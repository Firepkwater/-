import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    return 1 / (np.exp(-x)+1)
def sigmoidGradient(x):
    return sigmoid(x)*(1-sigmoid(x))
def sigmoidsecGradient(x):
    return (2*np.exp(-x) - np.exp(-x)*(1+np.exp(-x)))/(1+np.exp(-x))**3

def J(w,x,u,v):
    return v*sigmoid(w*x+u) + x*v*sigmoidGradient(w*x+u)*w - 2

x=np.linspace(0,1,10)

#初始化权重和偏置
w0 = np.random.rand(1)
v0 = np.random.rand(1)
u0 = np.ones(1)
loss = 1
alpha = 0.01#学习率
iter_count = 0
grad_w =[0]*10
grad_u = [0,0,0,0,0,0,0,0,0,0]
grad_v = [0,0,0,0,0,0,0,0,0,0]
while loss > 1.0e-9 and iter_count < 1000:
    loss = 0
    err1sum_w = 0
    err1sum_u = 0
    err1sum_v = 0
    for i in range(10):
        z0 = w0 * x[i] + u0

        grad_w[i] = ((x[i]**2)*v0*w0*sigmoidsecGradient(z0) + x[i]*v0*sigmoidsecGradient(z0) + v0*sigmoidGradient(z0)*x[i])*J(w0,x[i],u0,v0)
        err1sum_w = err1sum_w + grad_w[i]
        grad_u[i] = (v0*sigmoidGradient(z0) + x[i]*v0*w0*sigmoidsecGradient(z0))*J(w0,x[i],u0,v0)
        err1sum_u = err1sum_u + grad_u[i]
        grad_v[i] = (x[i]*sigmoidGradient(z0)*w0 + sigmoid(z0))*J(w0,x[i],u0,v0)
        err1sum_v = err1sum_v + grad_v[i]

    w0 = w0 - alpha * err1sum_w
    u0 = u0 - alpha * err1sum_u
    v0 = v0 - alpha * err1sum_v

    for i in range(10):
        z = w0 *x[i] + u0
        dy_dx = v0*sigmoid(z) + x[i]*v0*sigmoidGradient(z)*w0
        error = 0.5*(dy_dx - 2)**2
        loss = loss + error
    iter_count += 1

print("item_count:",iter_count)
print("loss:",loss)
print("w0:",w0,"u0:",u0,"v0:",v0)
fig = plt.figure()
x = np.linspace(0,1,10)
z=w0*x+u0
y1 = 1+x*v0*sigmoid(z)
y2 = 1+2*x
plt.ylim(0,4)
l1, =plt.plot(x,y1,color='red',linewidth='2.0',linestyle='-')
l2, =plt.plot(x,y2,color='blue',linewidth='1.0',linestyle='--')
plt.legend(handles = [l1,l2],labels=['trail solution','analytic solution'],loc='best')
plt.show()

